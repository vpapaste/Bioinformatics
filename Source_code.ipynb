{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import ProtParamData\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.SeqUtils import IsoelectricPoint\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, cross_val_predict, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "vectorizer = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the lists and dictionaries needed to \"hold\" the data\n",
    "class_names = [\"Cytosolic\", \"Mitochondrial\", \"Nuclear\", \"Secreted\"]\n",
    "labels = []\n",
    "mito_list = []\n",
    "cyto_list = []\n",
    "nucleus_list = []\n",
    "secreted_list = []\n",
    "blind_list = []\n",
    "mito_count_dict = defaultdict(float)\n",
    "cyto_count_dict = defaultdict(float)\n",
    "nucleus_count_dict = defaultdict(float)\n",
    "secreted_count_dict = defaultdict(float)\n",
    "full_dict = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list for every class and then a list that contains every class\n",
    "for seq_record in SeqIO.parse(\"cyto.fasta.txt\", \"fasta\"):\n",
    "    cyto_list.append(str(seq_record.seq))\n",
    "    labels.append(\"Cytosolic\")\n",
    "    \n",
    "for seq_record in SeqIO.parse(\"mito.fasta.txt\", \"fasta\"):\n",
    "    mito_list.append(str(seq_record.seq))\n",
    "    labels.append(\"Mitochondrial\")\n",
    "    \n",
    "for seq_record in SeqIO.parse(\"nucleus.fasta.txt\", \"fasta\"):\n",
    "    nucleus_list.append(str(seq_record.seq))\n",
    "    labels.append(\"Nuclear\")\n",
    "    \n",
    "for seq_record in SeqIO.parse(\"secreted.fasta.txt\", \"fasta\"):\n",
    "    secreted_list.append(str(seq_record.seq))\n",
    "    labels.append(\"Secreted\")\n",
    "\n",
    "for seq_record in SeqIO.parse(\"blind.fasta.txt\", \"fasta\"):\n",
    "    blind_list.append(str(seq_record.seq))\n",
    "\n",
    "full_list = cyto_list + mito_list + nucleus_list + secreted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the dataset statistics\n",
    "lengths_list = []\n",
    "mito_lengths = []\n",
    "cyto_lengths = []\n",
    "nucleus_lengths = []\n",
    "secreted_lengths = []\n",
    "list_of_lists = []\n",
    "stats_dict = {}\n",
    "for sequence in full_list:\n",
    "    lengths_list.append(len(sequence))\n",
    "for sequence in cyto_list:\n",
    "    cyto_lengths.append(len(sequence))\n",
    "for sequence in mito_list:\n",
    "    mito_lengths.append(len(sequence))\n",
    "for sequence in nucleus_list:\n",
    "    nucleus_lengths.append(len(sequence))\n",
    "for sequence in secreted_list:\n",
    "    secreted_lengths.append(len(sequence))\n",
    "\n",
    "list_of_lists.append(cyto_lengths)\n",
    "list_of_lists.append(mito_lengths)  \n",
    "list_of_lists.append(nucleus_lengths)\n",
    "list_of_lists.append(secreted_lengths)\n",
    "\n",
    "for i, category in enumerate(class_names):\n",
    "    stats_dict[str(category)] = [len(list_of_lists[i]),np.mean(list_of_lists[i]), np.std(list_of_lists[i]),\n",
    "                                 np.min(list_of_lists[i]), np.max(list_of_lists[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "full_count_dict = defaultdict(float)\n",
    "replaced_count_dict = defaultdict(float)\n",
    "analysed_list = []\n",
    "\n",
    "for sequence in full_list:\n",
    "    for aminoacid in sequence:\n",
    "        full_count_dict[aminoacid] += 1.0\n",
    "\n",
    "for i in range(len(full_list)):\n",
    "    full_list[i] = full_list[i].replace(\"X\", \"\")\n",
    "    full_list[i] = full_list[i].replace(\"U\", \"\")\n",
    "    full_list[i] = full_list[i].replace(\"B\", \"N\")\n",
    "    \n",
    "for sequence in full_list:\n",
    "    analysed_list.append(ProteinAnalysis(sequence))\n",
    "    for aminoacid in sequence:\n",
    "        replaced_count_dict[aminoacid] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that extracts features from a sequence of aminoacids\n",
    "# originally from http://biopython.org/\n",
    "def molar_extinction_coefficient(proteinobject):\n",
    "    \"\"\"\n",
    "    Calculate the molar extinction coefficient.\n",
    "    Calculates the molar extinction coefficient assuming cysteines\n",
    "    (reduced) and cystines residues (Cys-Cys-bond)\n",
    "    \"\"\"\n",
    "    num_aa = proteinobject.count_amino_acids()\n",
    "    mec_reduced = num_aa['W'] * 5500 + num_aa['Y'] * 1490\n",
    "    mec_cystines = mec_reduced + (num_aa['C'] // 2) * 125\n",
    "    \n",
    "    return(mec_reduced, mec_cystines)\n",
    "\n",
    "def feature_extractor(sequence):\n",
    "    \"\"\"\n",
    "    Create a dictionary of features derived from the amino acid chain\n",
    "    for each sequence. They names of the dictionary keys are representative\n",
    "    of the features.\n",
    "    \"\"\"\n",
    "    analysed_sequence = ProteinAnalysis(sequence)\n",
    "    \n",
    "    sequence_dict = defaultdict(float)\n",
    "    \n",
    "    \n",
    "    for aminoacid1 in replaced_count_dict.keys():\n",
    "        for aminoacid2 in replaced_count_dict.keys():       \n",
    "            sequence_dict[str(aminoacid1)+str(aminoacid2)] = 0.0\n",
    "    for aminoacid1 in replaced_count_dict.keys():\n",
    "        for aminoacid2 in replaced_count_dict.keys(): \n",
    "            for i in range(len(sequence)-1):\n",
    "                if (str(sequence[i]) + str(sequence[i+1]) == str(aminoacid1) + str(aminoacid2)):\n",
    "                    sequence_dict[str(sequence[i])+str(sequence[i+1])] +=1.0\n",
    "                \n",
    "    for aminoacid1 in replaced_count_dict.keys():\n",
    "        for aminoacid2 in replaced_count_dict.keys(): \n",
    "            sequence_dict[str(aminoacid1)+str(aminoacid2)] = sequence_dict[str(aminoacid1)+str(aminoacid2)]/len(sequence)\n",
    "    \n",
    "    for aminoacid in replaced_count_dict.keys():\n",
    "        sequence_dict[str(aminoacid) + \"_%\"] = sequence.count(aminoacid)/len(sequence)\n",
    "        sequence_dict[str(aminoacid) + \"_%\"+\"_F50\"] = sequence[:50].count(aminoacid)/len(sequence[:50])       \n",
    "        sequence_dict[str(aminoacid) + \"_%\"+\"_L50\"] = sequence[-50:].count(aminoacid)/len(sequence[-50:])\n",
    "\n",
    "    sequence_dict[\"aromaticity\"] = analysed_sequence.aromaticity() \n",
    "    \n",
    "    sequence_dict[\"instability_index\"] = analysed_sequence.instability_index()\n",
    "    \n",
    "    sequence_dict[\"gravy\"] = analysed_sequence.gravy()\n",
    "    \n",
    "    sequence_dict[\"isoelectric_point\"] = analysed_sequence.isoelectric_point()\n",
    "    \n",
    "    sequence_dict[\"helix\"] = analysed_sequence.secondary_structure_fraction()[0]\n",
    "\n",
    "    sequence_dict[\"turn\"] = analysed_sequence.secondary_structure_fraction()[1]\n",
    "      \n",
    "    sequence_dict[\"sheet\"] = analysed_sequence.secondary_structure_fraction()[2]\n",
    "\n",
    "    sequence_dict[\"length\"] = len(sequence)\n",
    "    \n",
    "    sequence_dict[\"molar_ex_coef1\"] = molar_extinction_coefficient(analysed_sequence)[0]\n",
    "    \n",
    "    sequence_dict[\"molar_ex_coef2\"] = molar_extinction_coefficient(analysed_sequence)[1]\n",
    "    \n",
    "    sequence_dict[\"hydrophobic_%\"] = (sequence.count('A') + sequence.count('I') + sequence.count('L') +\n",
    "                                       sequence.count('M') + sequence.count('P') + sequence.count('F') +\n",
    "                                       sequence.count('W') + sequence.count('V')) / len(sequence)    \n",
    "\n",
    "    sequence_dict['pos_charged_%'] = (sequence.count('H') + sequence.count('K') +\n",
    "                                           sequence.count('R')) / len(sequence)\n",
    "\n",
    "    sequence_dict['neg_charged_%']  =  (sequence.count('D') + sequence.count('E')) / len(sequence)\n",
    "\n",
    "    sequence_dict['polar_%']  =  (sequence.count('R') + sequence.count('N') + sequence.count('D') +\n",
    "                                sequence.count('E') + sequence.count('Q') + sequence.count('H') +\n",
    "                                sequence.count('K') + sequence.count('S') + sequence.count('T') +  \n",
    "                                sequence.count('Y')) / len(sequence)\n",
    "\n",
    "    sequence_dict['acidic_%']  =  (sequence.count('D') + sequence.count('E')) / len(sequence) \n",
    "\n",
    "    sequence_dict['basic_%']  =  (sequence.count('R') + sequence.count('H') + sequence.count('K')) / len(sequence) \n",
    "                                 \n",
    "    point_0 = IsoelectricPoint.IsoelectricPoint(analysed_sequence, analysed_sequence.count_amino_acids())\n",
    "    point = point_0._select_charged(analysed_sequence.count_amino_acids())\n",
    "    \n",
    "    sequence_dict[\"isoelectric_c\"] = point[\"C\"]\n",
    "    sequence_dict[\"isoelectric_Cterm\"] = point[\"Cterm\"]\n",
    "    sequence_dict[\"isoelectric_d\"] = point[\"D\"]\n",
    "    sequence_dict[\"isoelectric_e\"] = point[\"E\"]\n",
    "    sequence_dict[\"isoelectric_h\"] = point[\"H\"]\n",
    "    sequence_dict[\"isoelectric_k\"] = point[\"K\"]\n",
    "    sequence_dict[\"isoelectric_Nterm\"] = point[\"Nterm\"]\n",
    "    sequence_dict[\"isoelectric_r\"] = point[\"R\"]\n",
    "    sequence_dict[\"isoelectric_y\"] = point[\"Y\"]\n",
    "\n",
    "    sequence_dict[\"hydrophobic_%_F50\"] = (sequence[:50].count('A') + sequence[:50].count('I') + sequence[:50].count('L') +\n",
    "                                       sequence[:50].count('M') + sequence[:50].count('P') + sequence[:50].count('F') +\n",
    "                                       sequence[:50].count('W') + sequence[:50].count('V')) / len(sequence[:50])    \n",
    " \n",
    "    sequence_dict['pos_charged_%_F50'] = (sequence[:50].count('H') + sequence[:50].count('K') +\n",
    "                                           sequence[:50].count('R')) / len(sequence[:50])\n",
    "\n",
    "    sequence_dict['neg_charged_%_F50']  =  (sequence[:50].count('D') + sequence[:50].count('E')) / len(sequence[:50])\n",
    " \n",
    "    sequence_dict['polar_F50']  =  (sequence[:50].count('R') + sequence[:50].count('N') + sequence[:50].count('D') +\n",
    "                                sequence[:50].count('E') + sequence[:50].count('Q') + sequence[:50].count('H') +\n",
    "                                sequence[:50].count('K') + sequence[:50].count('S') + sequence[:50].count('T') +  \n",
    "                                sequence[:50].count('Y')) / len(sequence[:50])\n",
    "\n",
    "    sequence_dict['acidic_F50']  =  (sequence[:50].count('D') + sequence[:50].count('E')) / len(sequence[:50]) \n",
    "\n",
    "    sequence_dict['basic_F50']  =  (sequence[:50].count('R') + sequence[:50].count('H')\n",
    "                                    + sequence[:50].count('K')) / len(sequence[:50])                                 \n",
    "\n",
    "    sequence_dict[\"hydrophobic_%_L50\"] = (sequence[-50:].count('A') + sequence[-50:].count('I') + sequence[-50:].count('L') +\n",
    "                                       sequence[-50:].count('M') + sequence[-50:].count('P') + sequence[-50:].count('F') +\n",
    "                                       sequence[-50:].count('W') + sequence[-50:].count('V')) / len(sequence[-50:])    \n",
    "\n",
    "    sequence_dict['pos_charged_%_L50'] = (sequence[-50:].count('H') + sequence[-50:].count('K') +\n",
    "                                           sequence[-50:].count('R')) / len(sequence[-50:])\n",
    "\n",
    "    sequence_dict['neg_charged_%_L50']  =  (sequence[-50:].count('D') + sequence[-50:].count('E')) / len(sequence[-50:])\n",
    "  \n",
    "    sequence_dict['polar_%_L50']  =  (sequence[-50:].count('R') + sequence[-50:].count('N') + sequence[-50:].count('D') +\n",
    "                                sequence[-50:].count('E') + sequence[-50:].count('Q') + sequence[-50:].count('H') +\n",
    "                                sequence[-50:].count('K') + sequence[-50:].count('S') + sequence[-50:].count('T') +  \n",
    "                                sequence[-50:].count('Y')) / len(sequence[-50:])\n",
    "\n",
    "    sequence_dict['acidic_%_L50']  =  (sequence[-50:].count('D') + sequence[-50:].count('E')) / len(sequence[-50:]) \n",
    "\n",
    "    sequence_dict['basic_%_L50']  =  (sequence[-50:].count('R') + sequence[-50:].count('H')\n",
    "                                    + sequence[-50:].count('K')) / len(sequence[-50:])                                     \n",
    "\n",
    "\n",
    "    return sequence_dict\n",
    "\n",
    "\n",
    "def predict(sequence_list, model):\n",
    "    \"\"\"\n",
    "    define a function that predicts the class of a new sequence\n",
    "    \"\"\"\n",
    "    x_test = vectorizer.transform([feature_extractor(x) for x in sequence_list])\n",
    "    predictions = label_encoder.inverse_transform(model.predict(x_test))\n",
    "    probabilities = model.predict_proba(x_test)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "# define a function that computes and plots a confusion matrix (normilized or not)\n",
    "# code originally from http://scikit-learn.org/\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the sequences into numerical features usable by a classifier\n",
    "x_data = vectorizer.fit_transform([feature_extractor(x) for x in full_list]).toarray()\n",
    "# transform the labels into numerical values usable by a classifier\n",
    "y_data = label_encoder.fit_transform([y for y in labels])\n",
    "\n",
    "# split the dataset into train and test using stratified sampling\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1,\n",
    "                                                    random_state=13, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9222\n",
      "{0: 0.32574278898286707, 1: 0.14085881587508134, 2: 0.35935805682064625, 3: 0.17404033832140534}\n",
      "8299\n",
      "{0: 0.32570189179419207, 1: 0.14086034461983371, 2: 0.3593204000481986, 3: 0.17411736353777563}\n",
      "923\n",
      "{0: 0.32611050920910079, 1: 0.14084507042253522, 2: 0.35969664138678226, 3: 0.1733477789815818}\n"
     ]
    }
   ],
   "source": [
    "#dataset statistics for each class after the split (overall-train-test)\n",
    "unique, counts = np.unique(y_data, return_counts=True)\n",
    "data_distr = dict(zip(unique, counts/y_data.shape[0]))\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "test_distr = dict(zip(unique, counts/y_test.shape[0]))\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "train_distr = dict(zip(unique, counts/y_train.shape[0]))\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "test_distr = dict(zip(unique, counts/y_test.shape[0]))\n",
    "print(y_data.shape[0])\n",
    "print(data_distr)\n",
    "print(y_train.shape[0])\n",
    "print(train_distr)\n",
    "print(y_test.shape[0])\n",
    "print(test_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if tuning==True perform a grid search over 125 models using 10fold for each model\n",
    "# Warning: This might take a couple of days to complete\n",
    "tuning = False\n",
    "if tuning:\n",
    "    model = xgboost.XGBClassifier()\n",
    "    n_estimators = [200, 400, 500, 700, 1000]\n",
    "    max_depth = [7, 10, 12, 15, 20]\n",
    "    gamma = [i/10.0 for i in range (0,5)]\n",
    "    param_grid = dict(n_estimators=n_estimators, max_depth=max_depth, gamma=gamma)\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose=1)\n",
    "    grid_result = grid_search.fit(x_train, y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if cross_validation == True a 10 fold-cross validation is performed for a given model\n",
    "# and accurcaies are reported over the 10 folds\n",
    "cross_validation = False\n",
    "if cross_validation:\n",
    "    model = xgboost.XGBClassifier(n_estimators=1000, max_depth=10, n_jobs=-1)\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    accuracies = cross_val_score(model, x_train, y_train, cv=kfold, n_jobs=-1)\n",
    "    #predictions = cross_val_predict(model, x_train, y_train, cv=kfold, n_jobs=-1)\n",
    "    print(accuracies)\n",
    "    print(np.mean(accuracies))\n",
    "    print(np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the tuned parameters to train the model and do a final evaluation on the unseen test data\n",
    "model = xgboost.XGBClassifier(n_estimators=1000, max_depth=10, n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "accuracy = model.score(x_test, y_test)\n",
    "predictions = model.predict(x_test)\n",
    "probabilities = model.predict_proba(x_test)\n",
    "weighted_f1score = f1_score(y_test, predictions, average=\"weighted\")\n",
    "individual_f1scores = f1_score(y_test, predictions, average=None)\n",
    "print(accuracy)\n",
    "print(weighted_f1score)\n",
    "print(individual_f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the important features using build-in method .feature_importances\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = list(feature_extractor(full_list[0]).keys())\n",
    "feature_names = np.array(feature_names)\n",
    "feature_importance = np.array(feature_importance)\n",
    "index = feature_importance.argsort()\n",
    "sorted_names = feature_names[index]\n",
    "sorted_importances = feature_importance[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the 30 most significant features\n",
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "y_pos = np.arange(len(sorted_names[-30:]))\n",
    "ax.barh(y_pos, sorted_importances[-30:])\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(sorted_names[-30:])\n",
    "ax.set_xlabel(\"Feature Score\")\n",
    "ax.set_ylabel('Feature Name')\n",
    "ax.set_title(\"30 most significant features\")\n",
    "pyplot.show()\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 30 least significant features\n",
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "y_pos = np.arange(len(sorted_names[:30]))\n",
    "ax.barh(y_pos, sorted_importances[:30])\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(sorted_names[:30])\n",
    "ax.set_xlabel('Feature Score')\n",
    "ax.set_ylabel('Feature Name')\n",
    "ax.set_title('30 least significant features')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrices\n",
    "sns.set_style(\"dark\")\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                  title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                  title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y_train_binarized = label_binarize(y_train, classes=[0, 1, 2, 3])\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "n_classes = 4\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(xgboost.XGBClassifier(n_estimators=1000, max_depth=10, n_jobs=-1))\n",
    "y_score = classifier.fit(x_train, y_train_binarized).predict_proba(x_test)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "my_roc = dict()\n",
    "lw = 2\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    my_roc[i] = roc_auc_score(y_test_binarized[:, i], y_score[:, i])\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='brown', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='pink', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['grey', 'green', 'yellow', 'blue'])\n",
    "for category, color,i in zip(class_names, colors, range(n_classes)):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label=('ROC curve of class ' + str(category) + '(area = {1:0.2f})').format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print auc scores\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the final model using all data available (training+test)\n",
    "final_model = xgboost.XGBClassifier(n_estimators=1000, max_depth=10, n_jobs=-1)\n",
    "final_model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the labels of the unlabeled set and get the confidence scores and print them out\n",
    "blind_predictions, blind_probabilities = predict(blind_list, final_model)\n",
    "print(blind_predictions)\n",
    "print(blind_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions, confidence scores along with each proteins sequence id in a text file\n",
    "file = open(\"blind_predictions.txt\",\"w\") \n",
    "file.write(\"Id\" + \" \" + \"Prediction\" + \" \" + \"Confidence\" + \"\\n\")\n",
    "for seq_record, seq_prediction, seq_confidence in zip(SeqIO.parse(\"blind.fasta.txt\", \"fasta\"), blind_predictions, blind_probabilities):\n",
    "    #print(seq_record.id)\n",
    "    file.write(seq_record.id + \" \" + seq_prediction + \" \"+ str(np.max(seq_confidence)) + \"\\n\") \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
